Know the differences between Cloud SQL and Cloud Spanner, and when to use each. Service differentiators include access methods, the cost or speed of specific actions, sizes of data and how data is organized and stored

know how to identify technologies backwards from their properties. For example, which data technology offers the fastest ingest of data? Which one might you use for ingest of streaming data?

managed services still have some IT overhead. It doesn't completely eliminate the overhead or manual procedures, but it minimizes them compared with on prem solutions. Serverless services remove more of the IT responsibility.

is to understand the array of machine learning technologies offered on GCP and when you might want to use each.

Your exam tip here is to familiarize yourself with infrastructure services that show up commonly and data engineering solutions. Often they're employed because of key features they provide. For example, Cloud PubSub can hold a message for up to seven days

that it's good to know how data is stored and what purpose or use case is the storage or database optimized for. Flat serialized data is easy to work with, but it lacks structure and therefore meaning. If you want to represent data that has meaningful relationships, you need a method that not only represents the data but also the relationships.

know the hierarchy of objects within a data technology and how they relate to one another.

you should know how different services store data on how each method is optimised for specific use cases as previously mentioned, but also understand the key value of the approach and this case RDDs hide complexity and allow spark to make decisions on your behalf.There are a number of concepts that you should know about Cloud Dataflow.

spark is important because it does part of its pipeline processing in memory rather than copying from disk.

If the situation you're analyzing has data and BigQuery, and perhaps the business logic is better expressed in terms of functional code rather than SQL, you may want to run a spark job on the data. Cloud dataproc has connectors to all kinds of GCP resources. You can read from GCP sources and write to GCP sources and use cloud dataproc as the interconnecting glue.

about modifying the cloud data prop cluster if you need to modify the cluster, consider whether you have the right data processing solution. There are so many services available on Google Cloud, you might be able to use. 

Cloud Dataflow users use roles to limit access to only Dataflow resources not just the project.

A pipeline is a more maintainable way to organize Data Processing code than for example, an application running on it instance

Dataflow Templates open up new options for separation of work, and that means, better security and resource accountability.

access control, and Bigquery is at the project and the data set level. 

Here's a major design tip. Separate compute and processing from storage and database enables serverless

think about data in terms of the three V's volume, velocity and variety. How much, how often and how consistent. This will guide you to the best approach for ingesting the data. In brief, use GSU till for uploading files. Use the Storage transfer service when the data is in another location, such as another cloud, and use the transfer appliance when the data is too big to transfer electronically.

You need to know how long cloud pub/sub holds messages. It's up to seven days.

 This is the pattern you'll often see. Cloud pub/sub for data ingest, cloud dataflow for data processing and ETL, and BigQuery for interactive analysis. Exam tip, be able to recognize this pattern in case scenarios.

Tip: Be familiar with the common use cases and qualities of the different storage options. Each storage
system or database is optimized for different things -- some are best at atomically updating the data for
transactions. Some are optimized for speed of data retrieval but not for updates or changes. Some are
very fast and inexpensive for simple retrieval but slow for complex queries.

Tip: An important element in designing the data processing pipeline starts with selecting the appropriate
service or collection of services.

Tip: AI Platform Notebooks, Google Data Studio, BigQuery all have interactive interfaces. Do you know
when to use each?

 Pub/Sub and Dataflow together provide once, in-order, processing of possibly delayed or repeated
streaming data.

Be familiar with the common assemblies of services and how they are often used together: Dataflow,
Dataproc, BigQuery, Cloud Storage, and Pub/Sub

Tip: Technologically, Dataproc is superior to Open Source Hadoop, and Dataflow is superior to Dataproc.
However, this does not mean that the most advanced technology is always the best solution. You need to
consider the business requirements. The client might want to first migrate from the data center to the
cloud. Make sure everything is working (validate it). And only after they are confident with that solution, to
consider improving or modernizing.

Study ACID vs BASE!!!!!

Cloud Spanner is strongly typed and globally consistent. The two characteristics that distinguish it from Cloud SQL, are global consistent transactions and size. Cloud Spanner can work with much larger databases than Cloud SQL. Cloud SQL is fine if you can get by with a single database, but if your needs are such that you need multiple databases, Cloud Spanner is a great choice.

Example, if the exam question contains Data Warehouse, you should be thinking BigQuery as a candidate. If the case says something about large media files, you should immediately be thinking Cloud Storage.

data flow - window (fix, slide, session) and side inputs!

streaming pub/sub=>big query - 100k rows/table/sec. but might deliver the messages out of order. If you have a timestamp then Cloud data flow can remove duplicates and work out the order of messages. BigQuery is an inexpensive data store for tabular data. It's cost comparable with Cloud storage, so it makes sense to ingest into BigQuery and leave the data there.

Why Big-table and not Cloud Spanner, cost? Note that we can support a 100,000 queries per second with ten nodes and Big-table, but we would need about a 150 nodes in Cloud Spanner.


storage:
Data management is often influenced by business requirements. After the data has been used for the
"live" application, is it collected for reporting, for backup and recovery, for audits, or for legal compliance?
What are the changing business purposes of the data in different time frames?

"Effective use of managed services" … choose the right service and the correct settings/features for
specific use cases.

● Cloud Bigtable
● Cloud Spanner
● Cloud SQL
● BigQuery
● Cloud Storage
● Datastore
● Cloud Memorystore

pipeline:
What is Data cleansing? Data cleansing is improving the data quality through consistency. You could
use Dataprep to Extract, Transform, or Load (ETL). You could run a data transformation job on Dataproc.

Batch and streaming together? You should already be thinking "Dataflow"

Integrating with new data sources. You should be familiar with the connectors available between
services in the cloud, and common import/acquisition configurations.

Processing Infrastructure:
Testing and quality control and monitoring. You should be familiar with the common approaches to
testing that are used in production environments, such as A/B testing, and other rollout scenarios.
Likewise, there are operational and administrative monitoring elements for most services in the Cloud
Console, and statistical and log monitoring in Cloud Logging. Do you know how to enable and use Google
Cloud’s operations suite with common services?

For example, what are the three modes of the Natural Language API? The answer is sentiment analysis, entities, and syntax. 

This is the pattern for developing your own machine learning models. First, prepare the data. That means you gather the training data, clean the data, split it into pools or groups for different purposes. Then you select features and improve them with feature engineering next or the training data in an online location. That cloud machine learning engine can access, such as in cloud storage. Then you follow these steps. You use tensor flow to create the training application. You package it and then you can figure and start a cloud mill job.

One of the basic concepts of machine learning is correctable error. If you can make a guess about something like a value or a state, and if you know whether that guess was right or not.

Concepts like fast failure lifecycle and iterations become important in developing and refining a model.

study tensor flow!

mean square error

gradient descent is used to find the best parameters

RMSE??

classification error measure is cross entropy

You can use the evaluation dataset to determine if the model parameters are leading to overfitting. Overfitting, or memorizing your training dataset can be far worse than having a model that only adequately fits your data.

if the question says data is scarce, then you should be thinking independent test data or cross-validate our candidate answers.

To recap, you need to know regression and classification, labels, features, you need to know the progression of train, evaluate, and predict, and you need to understand some basic TensorFlow API course.

Tensor Flow estimator API?

study Kubeflow and AI Platform

Edge computing is the design of distributing processing in a strategic way so that model processing
is pushed closer to the inputs. For example, in IoT, doing Machine Learning processing closer to the IoT
sensors, by performing work in nearby datacenters or regions, is 'edge computing'.

unsupervised learning: neural networks, clustering, etc

In this example, a confusion matrix is used to make the data engineering choices understandable to the business users. So your exam tip is use a confusion matrix to describe the performance of classification models.

build (own model), buy (pre trained) or modify (auto ml)

Your exam tip. Consider using data where it is in place, maybe from cloud storage rather than using extract transform, load ETL.

Your exam tip grouping the work can be efficient and give additional control over the processing of the data. Don't get confused between initial proof of concept and production activities in general

Cases will refer to real world situations and your exam tip is to identify toy solutions. And distinguish them from real production solutions.

Your exam tip is better know about learning rate and hyperparameter tuning in machine learning. Performance is critical to practical solutions

Input data and data sources, I/O, how many bytes does your query read? Communication between nodes, shuffling, how many bytes does your query pass to the next stage? How many bytes does your query pass to each slot? Computation, how much CPU work does your query require? Outputs, also called materialization, how many bytes does your query write? Query anti-patterns, are your queries following SQL best practices.

know and understand normalization and denormalization and when to apply each to your data representation and design

Exam tip is to be suspicious of anything with ''Select All'' you need to understand the use of wildcards. 

Business Requirements
Technical Requirements
Technical Watchpoints (requirements or facts that indicate elements of a solution)
Proposed Solution

What if the policy on the resource only gives access to a single, lets say Cloud Storage Bucket, and restricts access to all other buckets? However, at the project level, a rule exist that grants access to all buckets in the project. Which rule wins? The more restrictive rule on the resource or the more general rule on the project. If the parent policy is less restrictive, it overrides a more restrictive resource policy. So in this case, the project policy wins.

cache big query:
exhibit non-deterministic behavior (for example, they use CURRENT_TIMESTAMP or RAND), 
if the table or view being queried has changed (even if the columns/rows of interest to the query are unchanged), 
if the table is associated with a streaming buffer (even if there are no new rows), 
if the query uses DML statements, or queries external data sources.

ST big query - spatial type

apache beam - combine ao inves de groupbykey

valid - muito nulos
consistency - valores duplicados
accuracy - joins errado, check com subtotal preco * qtd
complete - gaps nos datas, faltando um periodo por ex
uniform - mesma coluna com unidades diferentes

sql - row/record based storage
big query - column based storage

federated query

stack driver e tensorboardo to monitoring

It would be a good idea to make sure you know the main troubleshooting methods for each data engineering technology and service security and troubleshooting are the lateral subjects that crowd across all technologies

IAM -- Understand permissions and custom roles. Under what conditions are custom roles preferred
over standard predefined roles?

A lot of administration over resources is presented in the console. But a lot of runtime information
such as logs, performance, and so forth is presented and reported in Cloud Monitoring and Cloud
Logging. The tools within Google Cloud's operations suite provide information for troubleshooting both
functional and performance issues.

Establishing standard data quality at ingress using Dataprep by Trifacta or by running an ETL pipeline
can prevent many problems later in processing that would be difficult to troubleshoot.

Keep in mind the business purpose of the data processing. How resilient does the application need
to be? For example, financial transactions usually cannot be dropped and must not be duplicated. But a
statistical analysis might be equally valid if a small amount of data is lost. These assumptions influence
the approach to rerunning failed jobs.

Where is the official authoritative data (sometimes called the source of truth) and where are the
replicas? How frequently does data need to be shared or updated? Can smaller parts of the data be
synchronized to reduce costs?

Where is the data stored? Where is the data going to be processed? Can data storage and data
processing be in locations near one another?

When will the data need to be exported? How difficult and expensive will it be to make this happen?
For example, you might want to store data in a different location or in a different type of storage to meet
business requirements for portability.

Transfer Appliance

Storage Transfer Service

IOT Core


Partner Interconnect

Cloud Speech-to-Text API, and send requests in an asynchronous mode.

Use a row key of the form <sensorid>#<timestamp>. - hotspotting

If you thoroughly understand the ‘How-to’ and ‘Concepts‘ sections of Google Cloud documentation, you easily have 70% of what it takes to clear GCP certification


-------------------
Google Cloud Storage
-------------------

#0 is a special identifier for the most recent version of an object. #0 is useful to add when the name of the object ends in a string that would otherwise be interpreted as a generation number.

Geo-redundancy ensures maximum availability of your data, even in the event of large-scale disruptions, such as natural disasters. For dual-regions, geo-redundancy is achieved using two specific regions. For multi-regions, geo-redundancy is achieved using any combination of data centers within the specified multi-region, which may include data centers that are not explicitly listed as available regions.

Objects are immutable, which means that an uploaded object cannot change throughout its storage lifetime

Updating the same object more frequently than once per second may result in 429 Too Many Requests errors.

The following command synchronizes data between an Amazon S3 bucket and a Cloud Storage bucket:
gsutil rsync -d -r s3://my-aws-bucket gs://example-bucket

Cloud Storage is a multi-tenant service, meaning that users share the same set of underlying resources. 
Approximately 1000 object write requests per second, which includes uploading, updating, and deleting objects.
Approximately 5000 object read requests per second, which includes listing objects, reading object data, and reading object metadata.

As a bucket approaches its IO capacity limit, Cloud Storage typically takes on the order of minutes to detect and accordingly redistribute the load across more servers. Consequently, if the request rate on your bucket increases faster than Cloud Storage can perform this redistribution, you may run into temporary limits, specifically higher latency and error rates. 

In order to maintain a high request rate, avoid using sequential names. Using completely random object names will give you the best load distribution.

In order to be eligible for decompressive transcoding, an object must meet two criteria:
The file is gzip-compressed when stored in Cloud Storage.
The associated metadata includes Content-Encoding: gzip.

While decompressive transcoding allows objects to be stored in Cloud Storage in a compressed state, saving space and costs, charges for downloading the object are based on its decompressed size, because that is the size of the served object. For more information, see the pricing guide.

To prevent the above situation from occurring, you should begin by getting the metadata for file.txt in order to determine its current generation. You then send the delete request with an if-generation-match precondition that uses the generation number. Using the precondition ensures that only the object with that specific generation number gets deleted, regardless of when the delete request reaches Cloud Storage or how many times the delete request with the precondition is sent. With the if-generation-match precondition, any unintended attempts to mutate a different generation of file.txt fail with the response code 412 Precondition Failed.

Important: Cached objects that are publicly readable might not exhibit strong consistency. See Cache control and consistency for details.

The following operations are eventually consistent: Granting access to or revoking access from resources.

The Cloud Storage access control system includes the ability to specify that objects are publicly readable. Make sure you intend for any objects you write with this permission to be public. Once "published", data on the Internet can be copied to many places, so it's effectively impossible to regain read control over an object written with this permission.

If you need to make content available securely to users who don't have Google accounts we recommend you use signed URLs. For example, with signed URLs you can provide a link to an object and your application's customers do not need to authenticate with Cloud Storage to access the object. When you create a signed URL you control the type (read, write, delete) and duration of access.

If you use Compute Engine instances with processes that POST to Cloud Storage to initiate a resumable upload, then you should use Compute Engine instances in the same locations as your Cloud Storage buckets. You can then use a geo IP service to pick the Compute Engine region to which you route customer requests, which helps keep traffic localized to a geo-region.

A retention policy that specifies a retention period can be placed on a bucket. An object in the bucket cannot be deleted or replaced until it reaches the specified age.

Caution: Object Versioning does not protect your data if you delete the entire bucket.

If not otherwise specified in your request, buckets are created in the US multi-region and have a default storage class of Standard Storage.

You cannot change the name of an existing bucket. Instead, you should create a new bucket with the desired name and move the contents from the old bucket to the new bucket. See Moving and Renaming Buckets for a step-by-step guide.

Unlike the "coldest" storage services offered by other Cloud providers, your data is available within milliseconds, not hours or days.

Operations that have both a source bucket and a destination bucket, such as a copy or rewrite, charge to the project that contains the source bucket

The mutability of metadata varies: some metadata you can edit at any time, some metadata you can only set at the time the object is created, and some metadata you can only view. For example, you can edit the value of the Cache-Control metadata at any time, but you can only assign the storageClass metadata when the object is created or rewritten, and you cannot directly edit the value for the generation metadata, though the generation value changes when the object is replaced.

Once you enable uniform bucket-level access, you have 90 days to switch back to fine-grained access before uniform bucket-level access becomes permanent.


neural networks
wide - many features - memory
tall/deep - many layers - generalization
wide and tall - recomend systems
